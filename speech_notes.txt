Cybernetics 40-60
1943: McCulloch-Pitts neuron: summation over weighted inputs and output function of the sum: can simulate logic gates of AND/OR
1958: Mark I Perception by Frank Rosenblatt step activatoin function, biological inspiration 
	in that neurno fired when enough input signals, for classification where tweak weights,
	however not does not exist solution

Great intro
Pause points, ask questions, figure when something is too technical

When neural nets started, I get the sense that the scientists would make AI rather quickly.
Their foray into the problem, they called the approach cybernetics. It reminds me about the
ominous Cybermen from Doctor Who.
In 1943, MucCulloch (neuroscientist) and Pitts (logician) created the eponymous McCulloch-Pitts neuron.
	There is only 1 neuron that performs binary classification. At the time, the non-linear activation
	function was solely a step function. The notion was biologically inspired where there were be many
	features from dendrites and if there was sufficient energy past critical mass, the neuron would fire--
	hence, the step function representing on or off. The neuron can simulate AND and OR logic gates.
In 1958, Rosenblatt created the Mark I Perception that added a training method that helped
	determine optimal ways to create essentially the decision bondary that allowed
	the neuron to fire.
By 1969, Marvin Minsky at MIT published the book "Perceptions" that showed that a single
	neuron could not simulate an XOR gate due to linear separability. However,
	he did not say that multiple layers of neurons could not replicate an XOR gate.
	Nonetheless, the damage was done. Then came the 1st AI Winter and cybernetics was 
	out of fashion.



Connectionsim 80-90s
1974: Backpropogation dicovered by Werbos for PhD--basically chain rule--for MLP but unknown
1986: Hinton and others repopularized backprop through "Learning internal representations by error propagation" 
	to show neural networks were traininable
1989: "Multilayer feedforward networks are universal approximators", just infinite neurons (memory) and computation
	1 hidden layer
1989: Yann LeCunn at Bell Labs wrote "Backpopagation Applied to Handwritten Zip Code Recognition"--convolution
	for translation invariance and reducing number of weights, local feature extraction, also subsampling/pooling,
	also biological inspiration simple cells to complex cell

Luckily for us, Geoff Hinton had been working on neural nets for a long time and wasn't going to give up.
During this time, the repopularized neural networks was rebranded as connectionism.
In 1986, Hinton showed rediscovered a method called backprop that make neuron networks trainable, 
	basically using derivatives to update weights.
1989 was a momentous year: the Universal Approximation Theorem proved that a neural net
	with a single layer can fit any function. However, in practice this is still a hard thing to do.
	It was proved that the sigmoid and later other functions would be a sufficient activation function.
Yann LeCun publishes ... which invents convolutional layer and pooling. The pooling wasn't maxpooling but
	the idea of downsampling--dimensionality reduction was there. Basically convolutional filters
	perform automatical feature engineering. For example, in an image, the first convolutional layer
	find simple features such as edges and curvatives. The second convolutional layer puts these features
	together to create a more abstract idea: circle or a square. The third convolution layer is more
	abstract in that it detects wheels and tables. In technical terms, convolutional filters features
	that are translation invariant--basically, it can find features anywhere in the image. That
	way, we say a lot of RAM by having fewer weights in the layer.
Waibel creates the Time-Delay neural net that takes in sequential data, a precursor to the RNN
Neural nets also gets more interesting outside of solely image classification.
In 1992, TD-Gammon became a world-class backmgammon AI, 1995 Sebastian Thrun created a chess AI.
	However, do note that this chess AI was not competitive with other simplier AI. The reason being
	that backgammon is a more greedy game between rounds--you need less strategy. Whereas chess
	requires lots of time steps in strategy.
1997: Schmidhuber created the LSTM, a variant of the standard RNN. Basically, standard recurrent layer
	is useful for training sequential data like speech or video. However, a problem it encounters
	during training is it's hard to update weights after 7 steps back in time due to a problem
	called vanishing or exploding gradients. LSTMs is the de-facto technique that allows keeping
	memory for actions taken 1000 steps back in time.
1998: Yan LeCun develops LeNet--convent for reading checks. At one time, it is said that this net
	was reading 10% of all checks in ATMs.
However, something else was progressing quite well. SVMs had become in vogue due to progress
	in training and doing well on benchmark. Really SVMs can be simulated by 2-layer neural network
	but they were taking off while connectionism fell to the wayside.

Fortunately, Hinton made the come back again in 2006 with the rebranding deep learing. Oh snap!
In 2006: Hinton showed how pretraining techniques with restricted boltzmann machines could train
	a deep belief net. Basically, neural nets with many layers can be trained.
2009 is pretty cool too. Andrew Ng at Stanford figured out how to use GPUs to parallelize computation.
	We have to thank gamers for commoditizing GPUs.
2010: In ReLu activation function superseded sigmoid and tanh, which also reduces the exploding
	and vanishing gradient problem. Xavier initialization is a neat trick to start the weights
	intelligently, so you don't spend forever backpropping just to get the first layer's weights
	correct.
2012 big year: Google, Microsoft, IBM, and Hinton's lab came together for speech recognition.
	Dropout was developed to simulate ensembling on 1 instance of a neural net. Basically reduces overfitting,
		probably better regularization than l1 and l2 penalization.
	ImageNet is what I think is what pushed neural nets to mainstream... data science.
		AlexNet significantly outperformed SVMs--take that--by such a large margin
		that neural nets couldn't be ignored. You guessed it Geoff Hinto lead this team.
Neural nets are becoming the new electricty--there's exponential take up in applications and research.
	Right now, YOLO--You Only Look Once--detects objects in effectively real time.
Baidu is using AlphaGo's architecture to play Starcraft.
Uber's got self-driving cars.
The future is here.






neural networks for unsupervised: autoencoders, DBN, reinforcement learning, robotics
1992: TD-Gammon: world class--apparently less temporal than Chess or Go
1989: Waibel and others created Time-delay neural networks "Phoneme recognition using time-delay neural networks"
	similar to CNN--still feedforward, now RNN are superior
1995: Sebastian Thrun: "Learning to Play the Game of Chess" though not good, too slow
1997: Schmidhuber introduced LSTM to train RNN to resolve exploding and vanishing gradient
AI Winter--SVM take over--comparable to 2 layer neural net
LeNet for reading checks

during AI Winter: Geof Hinton, Yoshua Bengio, Yann LeCun survived due to funding from Canadian Institute for Advanced Research CIFAR

Deep Learning 06+
2006: rebranded as deep learning where Hinton introduced unsupervised pretraining and deep belief nets
2006: "A fast learning algorithm for deep belief nets" paper that showed that nets with many layers can be trained well
2009: "large scale deep unsupervised learning using graphics processors" use GPUs, reduced weeks of work into days
2010: "Understanding the difficulty of training deep feedforward neural networks" to get ReLu and Xavier initialization
2012: Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Resaerch Groups" Google, Microsoft, IBM, and Hinton's lab
2012: "Improving neural networks by preventing co-adaptation of feature detectors" dropout, simulates ensemble of neural networks
2012: ImageNet with AlexNet: GPU implementation, ReLu, dropout
2015: Geoff Hinto says why didn't backpropagation work in 1986?
	Our labeled datasets were thousands of times too small.
	Our computers were millions of times too slow.
	We initialized the weights in a stupid way.
	We used the wrong type of non-linearity.

momentum, NAG, Adagrad, Adadelta, RMSProp, Adam, AdaMax, Nadam
TensorFlow (2015), Keras (2015), Caffe (2013), Torch (2002), DL4J (2015), MXNet (2016), CNTK (2016),Theano (2010)


Last Thoughts:
don't count the Canadians out
Fei Fei Li's struggle with gathering data
activation functions shouldn't be linear or else it can be compressed to 1 layer, hence not perhap matrix multiplication
hidden layers find features, automatic feature engineering
linear algebra for feedforward, calculs for backrpopogation
don't be a hero!
CapsuleNet actually biological
Brain doesn't do backprop


Sources:
https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html
http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/
https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/